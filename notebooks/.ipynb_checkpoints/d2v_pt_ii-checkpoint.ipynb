{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "pd.set_option(\"display.max_columns\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_data():\n",
    "    df = pd.read_csv('../data/rt_data_dump.csv')\n",
    "    # Drop duplicate columns\n",
    "    df.drop(['Unnamed: 0', 'rt_id.1', '_id'], axis=1, inplace=True)\n",
    "    # Drop non-text annotations\n",
    "    img_only_idxs = df[df['tate_text'].isna()].index\n",
    "    df.drop(img_only_idxs, axis=0, inplace=True)\n",
    "    # All songs are \"False\" -- therefore, this doesn't add anything!\n",
    "    df.drop('hot_song', axis=1, inplace=True)\n",
    "    # Create standardized \"votes\" feature (takes pageviews into account)\n",
    "    df['votes_per_1000views'] = (100000 * df['votes_total'] / df['pageviews']).round(2)\n",
    "    # New features for the number of characters in annotations/referents\n",
    "    df['chars_in_tate'] = df['tate_text'].str.len()\n",
    "    df['chars_in_referent'] = df['ref_text'].str.len()\n",
    "    # list of words, in order, for referents/annotations\n",
    "    df['ref_word_lst'] = df['ref_text'].str.lower().str.split()\n",
    "    df['tate_word_lst'] = df['tate_text'].str.lower().str.split()\n",
    "    # word count for referents/annotations\n",
    "    df['ref_word_cnt'] = df['ref_word_lst'].str.len()\n",
    "    df['tate_word_cnt'] = df['tate_word_lst'].str.len()\n",
    "\n",
    "    # Removing Verse/Speaking Tags, Etc...\n",
    "    short_refs = df[df['ref_word_cnt'] <= 3]['ref_text'].unique()\n",
    "    tags_to_remove = []\n",
    "    short_refs_to_keep = []\n",
    "\n",
    "    for ref in short_refs:\n",
    "        if ref[0] == '[' and ref[-1] == ']':\n",
    "            tags_to_remove.append(ref)\n",
    "        else:\n",
    "            short_refs_to_keep.append(ref)\n",
    "\n",
    "    # COMPLETELY REMOVE\n",
    "    add_to_remove = ['Intro:', 'ENSEMBLE', 'JEFFERSON', 'Verse 2: Eminem', '[Chorus: KING GEORGE', '*Space Bar Tap*', 'BURR', 'LEE', '(Guitar Solo)', '(21st-Century schizoid man)']\n",
    "    # CHANGE/EDIT\n",
    "    edit_values = ['[HAMILTON]\\n No', '[HAMILTON]\\n Sir!', '[HAMILTON]\\n Ha', '[HAMILTON]\\n What?']\n",
    "    # OK\n",
    "    ok_keep = ['Mr. President', 'Mr. Vice President:', '“President John Adams”', 'Hamilton', 'Maty Noyes']\n",
    "\n",
    "    replace_dict = {'[HAMILTON]\\n No':'No', '[HAMILTON]\\n Sir!': 'Sir!', '[HAMILTON]\\n Ha': 'Ha', '[HAMILTON]\\n What?': 'What?'}\n",
    "\n",
    "    edit_idxs = []\n",
    "    for bad_ref in edit_values:\n",
    "        mask = df['ref_text'] == bad_ref\n",
    "        bad_idxs = list(df[mask].index)\n",
    "        for i in bad_idxs:\n",
    "            edit_idxs.append(i)\n",
    "\n",
    "    df['ref_text'].replace(replace_dict, inplace=True)\n",
    "\n",
    "    for i in add_to_remove:\n",
    "        tags_to_remove.append(i)\n",
    "        short_refs_to_keep.remove(i)\n",
    "\n",
    "    rt_idxs_to_drop = []\n",
    "    for bad_ref in tags_to_remove:\n",
    "        mask = df['ref_text'] == bad_ref\n",
    "        bad_idxs = list(df[mask].index)\n",
    "        for i in bad_idxs:\n",
    "            rt_idxs_to_drop.append(i)\n",
    "\n",
    "    df.drop(rt_idxs_to_drop, axis=0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def perform_ttsplit(df):\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "#     df_train.to_csv('../data/genius_data_train_319.csv')\n",
    "#     df_test.to_csv('../data/genius_data_test_319.csv')\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_tate_dfs(df_train, df_test):\n",
    "    ref_df_train = df_train[['ref_text', 'rt_id']]\n",
    "    tate_df_train = df_train[['tate_text', 'rt_id']]\n",
    "\n",
    "    ref_df_test = df_test[['ref_text', 'rt_id']]\n",
    "    tate_df_test = df_test[['tate_text', 'rt_id']]\n",
    "\n",
    "    ref_df_train.reset_index(drop=True, inplace=True)\n",
    "    tate_df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    ref_df_test.reset_index(drop=True, inplace=True)\n",
    "    tate_df_test.reset_index(drop=True, inplace=True)\n",
    "    # (tate_df_train['rt_id'] == ref_df_train['rt_id']).all()\n",
    "    # (tate_df_test['rt_id'] == ref_df_test['rt_id']).all()\n",
    "    return ref_df_train, tate_df_train, ref_df_test, tate_df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GONNA USE THIS TUTORIAL FOR REST OF ATTEMPT:\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb\n",
    "def isolate_corpuses(ref_df_train, tate_df_train, ref_df_test, tate_df_test):\n",
    "    refs_train = ref_df_train['ref_text']\n",
    "    tates_train = tate_df_train['tate_text']\n",
    "\n",
    "    refs_test = ref_df_test['ref_text']\n",
    "    tates_test = tate_df_test['tate_text']\n",
    "    return refs_train, refs_test, tates_train, tates_test\n",
    "\n",
    "def make_rt_doc_idx_dicts(ref_df_train, ref_df_test):\n",
    "    rt_to_doc_idx_train = ref_df_train['rt_id']\n",
    "    rt_to_doc_idx_test = ref_df_test['rt_id']\n",
    "\n",
    "    rt_doc_idx_train_dict = rt_to_doc_idx_train.to_dict()\n",
    "    rt_doc_idx_test_dict = rt_to_doc_idx_test.to_dict()\n",
    "    return rt_doc_idx_train_dict, rt_doc_idx_test_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(doc_series, tokens_only=False):\n",
    "    # with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "    for i, line in enumerate(doc_series):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "\n",
    "def get_rt_tt_corpuses(refs_train, refs_test, tates_train, tates_test):\n",
    "    train_tate_corpus = list(read_corpus(tates_train))\n",
    "    test_tate_corpus = list(read_corpus(tates_test, tokens_only=True))\n",
    "\n",
    "    train_refs_corpus = list(read_corpus(refs_train))\n",
    "    test_refs_corpus = list(read_corpus(refs_test, tokens_only=True))\n",
    "    return train_tate_corpus, test_tate_corpus, train_refs_corpus, test_refs_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(model, mod_corpus):\n",
    "    ranks = []\n",
    "    second_ranks = []\n",
    "    n_training_docs = len(mod_corpus)\n",
    "    for doc_id in range(n_training_docs):\n",
    "        inferred_vector = model.infer_vector(mod_corpus[doc_id].words)\n",
    "        sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "        rank = [docid for docid, sim in sims].index(doc_id)\n",
    "        ranks.append(rank)\n",
    "        second_ranks.append(sims[1])\n",
    "    # Let's count how each document ranks with respect to the training corpus\n",
    "    rank_counter = collections.Counter(ranks)  # Results vary between runs due to random seeding and very small corpus\n",
    "    cnt_correct_self_similarity_docs = rank_counter[0]\n",
    "    perc_correct_similarity = cnt_correct_self_similarity_docs / n_training_docs\n",
    "    greater_than_95 = perc_correct_similarity >= 0.95\n",
    "    return ranks, second_ranks, rank_counter, perc_correct_similarity, greater_than_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = get_and_clean_data()\n",
    "df_train, df_test = perform_ttsplit(cleaned_df)\n",
    "ref_df_train, tate_df_train, ref_df_test, tate_df_test = get_ref_tate_dfs(df_train, df_test)\n",
    "refs_train, refs_test, tates_train, tates_test = isolate_corpuses(ref_df_train, tate_df_train, ref_df_test, tate_df_test)\n",
    "rt_doc_idx_train_dict, rt_doc_idx_test_dict = make_rt_doc_idx_dicts(ref_df_train, ref_df_test)\n",
    "train_tate_corpus, test_tate_corpus, train_refs_corpus, test_refs_corpus = get_rt_tt_corpuses(refs_train, refs_test, tates_train, tates_test)\n",
    "\n",
    "ref_model1 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "ref_model2 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=80)\n",
    "ref_model3 = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=80)\n",
    "\n",
    "tate_model1 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "tate_model2 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=80)\n",
    "tate_model3 = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=80)\n",
    "\n",
    "rt_model1 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "rt_model2 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=80)\n",
    "rt_model3 = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=80)\n",
    "\n",
    "ref_models = [ref_model1, ref_model2, ref_model3]\n",
    "ref_mod_text = ['RM1(vs=50, mc=2, e=40)', 'RM2(vs=50, mc=2, e=80)', 'RM3(vs=100, mc=1, e=80)']\n",
    "tate_models = [tate_model1, tate_model2, tate_model3]\n",
    "tate_mod_text = ['TM1(vs=50, mc=2, e=40)', 'TM2(vs=50, mc=2, e=80)', 'TM3(vs=100, mc=1, e=80)']\n",
    "rt_models = [tate_model1, tate_model2, tate_model3]\n",
    "rt_mod_text = ['RTM1(vs=50, mc=2, e=40)', 'RTM2(vs=50, mc=2, e=80)', 'RTM3(vs=100, mc=1, e=80)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_examples(model, mod_corpus, doc_id):\n",
    "    inferred_vector = model.infer_vector(mod_corpus[doc_id])\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(mod_corpus[doc_id].words)))\n",
    "    print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "    for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "        print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(mod_corpus[sims[index][0]].words)))\n",
    "\n",
    "def compare_second_most_similar_doc_examples(mod_corpus, mod_second_ranks):\n",
    "    # We can run the next cell repeatedly to see a sampling other target-document comparisons.\n",
    "    # Pick a random document from the corpus and infer a vector from the model\n",
    "    doc_id = random.randint(0, len(mod_corpus) - 1)\n",
    "    # Compare and print the second-most-similar document\n",
    "    print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(mod_corpus[doc_id].words)))\n",
    "    sim_id = second_ranks[doc_id]\n",
    "    print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(modcorpus[sim_id[0]].words)))\n",
    "\n",
    "def random_model_tests(model, train_corpus, test_corpus):\n",
    "    # Testing the Model\n",
    "    # Using the same approach above, we'll infer the vector for a randomly chosen test document, and compare the document to our model by eye.\n",
    "    # Pick a random document from the test corpus and infer a vector from the model\n",
    "    doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "    inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    # Compare and print the most/median/least similar documents from the train corpus\n",
    "    print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "    print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "    for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "        print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n",
    "    return sims\n",
    "\n",
    "def get_cos_sim_for_best_worst_rt_pairs(df, base_model, r_corpus_train, r_corpus_test, t_corpus_train, t_corpus_test):\n",
    "    # 3 rt_ids for annotations we know to be particularly \"bad\"\n",
    "    bottom_3_rtid = [rt_id for rt_id in df.nsmallest(3, 'votes_per_1000views')['rt_id']]\n",
    "    # 3 rt_ids for annotations we know to be \"good\", minus the one that's only annotating a '[VERSE]' tag\n",
    "    top_3_rtid = [rt_id for rt_id in df.nlargest(3, 'votes_per_1000views')['rt_id']]\n",
    "    top = []\n",
    "    bottom = []\n",
    "    for top_or_bottom in [bottom_3_rtid, top_3_rtid]:\n",
    "        doc_ids = []\n",
    "        train_or_tests = []\n",
    "        cs = []\n",
    "        for rt_id in top_or_bottom:\n",
    "            if rt_id in r_corpus_train['rt_id']:\n",
    "                mask = r_corpus_train['rt_id'] == rt_id\n",
    "                doc_ids.append(r_corpus_train[mask].index[0])\n",
    "                train_or_tests.append('train')\n",
    "            else:\n",
    "                mask = r_corpus_test['rt_id'] == rt_id\n",
    "                doc_ids.append(r_corpus_test[mask].index[0])\n",
    "                train_or_tests.append('test')\n",
    "        for idx, doc_id in enumerate(doc_ids):\n",
    "            # let's start with using tate model\n",
    "            if train_or_tests[idx] == 'train':\n",
    "                r_inf_vec = base_model.infer_vector(r_train_corpus[doc_id].words, epochs=base_model.epochs).reshape(-1, 1)\n",
    "                t_inf_vec = base_model.infer_vector(t_train_corpus[doc_id].words, epochs=base_model.epochs).reshape(-1, 1)\n",
    "            else:\n",
    "                r_inf_vec = base_model.infer_vector(r_test_corpus[doc_id], epochs=base_model.epochs).reshape(-1, 1)\n",
    "                t_inf_vec = base_model.infer_vector(t_test_corpus[doc_id], epochs=base_model.epochs).reshape(-1, 1)\n",
    "            # might need to just do straight up np.cosine similarity calc between vecs\n",
    "            rt_iv_cs = 1 - cosine(r_inf_vec, t_inf_vec)\n",
    "            cs.append(rt_iv_cs)\n",
    "        mean_cs = np.array(rt_iv_cs).mean()\n",
    "        if top_or_bottom == bottom_3_rtid:\n",
    "            bottom.append([mean_cs, cs])\n",
    "        else:\n",
    "            top.append([mean_cs, cs])\n",
    "    return top, bottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2674, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(669, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING: RM1(vs=50, mc=2, e=40)\n",
      "CPU times: user 3.67 s, sys: 906 ms, total: 4.57 s\n",
      "Wall time: 3.11 s\n",
      "Model Self-Similarity Test Passed?: False\n",
      "Model % Self-Similar: 0.8765893792071803\n",
      "Test Document (503): «if pull up with kerry washington that gon be an enormous scandal could have naomi campbell and still might want me stormy daniels»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (1908, 0.49907630681991577): «damn girl you re mean they be startin shit but it yo world»\n",
      "\n",
      "MEDIAN (1113, -0.1142084151506424): «under the assumption love is dead already»\n",
      "\n",
      "LEAST (549, -0.4498591125011444): «so when you play this song rewind the first verse about me abusing my power so you can hurt about me and her in the shower whenever she horny about me and her in the after hours of the morning about her baby daddy currently serving life»\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a832586cb0d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrandom_model_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_refs_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_refs_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cos_sim_for_best_worst_rt_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_refs_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_refs_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tate_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tate_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-60ed569f26f4>\u001b[0m in \u001b[0;36mget_cos_sim_for_best_worst_rt_pairs\u001b[0;34m(df, base_model, r_corpus_train, r_corpus_test, t_corpus_train, t_corpus_test)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrt_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_or_bottom\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mrt_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_corpus_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rt_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_corpus_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rt_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrt_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mdoc_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_corpus_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "for idx, ref_mod in enumerate(ref_models):\n",
    "    model_header = ref_mod_text[idx]\n",
    "    print(\"EVALUATING:\", model_header)\n",
    "    ref_mod.build_vocab(train_refs_corpus)\n",
    "    %time ref_mod.train(train_refs_corpus, total_examples=ref_mod.corpus_count, epochs=ref_mod.epochs)\n",
    "\n",
    "    mod_ranks, mod_second_ranks, mod_rank_counter, perc_correct_similarity, greater_than_95 = assess_model(ref_mod, train_refs_corpus)\n",
    "    print(\"Model Self-Similarity Test Passed?:\", greater_than_95)\n",
    "    print(\"Model % Self-Similar:\", perc_correct_similarity)\n",
    "\n",
    "    doc_id = 4\n",
    "#     print_most_similar_examples(ref_mod, train_refs_corpus, doc_id)\n",
    "#     compare_second_most_similar_doc_examples(train_refs_corpus, mod_second_ranks)\n",
    "\n",
    "    random_model_tests(ref_mod, train_refs_corpus, test_refs_corpus)\n",
    "\n",
    "    top, bottom = get_cos_sim_for_best_worst_rt_pairs(cleaned_df, ref_mod, train_refs_corpus, test_refs_corpus, train_tate_corpus, test_tate_corpus)\n",
    "    print(top)\n",
    "    print(bottom)\n",
    "    print('NOW, ONTO THE NEXT MODEL!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
