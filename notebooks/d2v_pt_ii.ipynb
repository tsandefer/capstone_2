{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "pd.set_option(\"display.max_columns\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_data():\n",
    "    df = pd.read_csv('../data/rt_data_dump.csv')\n",
    "    # Drop duplicate columns\n",
    "    df.drop(['Unnamed: 0', 'rt_id.1', '_id'], axis=1, inplace=True)\n",
    "    # Drop non-text annotations\n",
    "    img_only_idxs = df[df['tate_text'].isna()].index\n",
    "    df.drop(img_only_idxs, axis=0, inplace=True)\n",
    "    # All songs are \"False\" -- therefore, this doesn't add anything!\n",
    "    df.drop('hot_song', axis=1, inplace=True)\n",
    "    # Create standardized \"votes\" feature (takes pageviews into account)\n",
    "    df['votes_per_1000views'] = (100000 * df['votes_total'] / df['pageviews']).round(2)\n",
    "    # New features for the number of characters in annotations/referents\n",
    "    df['chars_in_tate'] = df['tate_text'].str.len()\n",
    "    df['chars_in_referent'] = df['ref_text'].str.len()\n",
    "    # list of words, in order, for referents/annotations\n",
    "    df['ref_word_lst'] = df['ref_text'].str.lower().str.split()\n",
    "    df['tate_word_lst'] = df['tate_text'].str.lower().str.split()\n",
    "    # word count for referents/annotations\n",
    "    df['ref_word_cnt'] = df['ref_word_lst'].str.len()\n",
    "    df['tate_word_cnt'] = df['tate_word_lst'].str.len()\n",
    "\n",
    "    # Removing Verse/Speaking Tags, Etc...\n",
    "    short_refs = df[df['ref_word_cnt'] <= 3]['ref_text'].unique()\n",
    "    tags_to_remove = []\n",
    "    short_refs_to_keep = []\n",
    "\n",
    "    for ref in short_refs:\n",
    "        if ref[0] == '[' and ref[-1] == ']':\n",
    "            tags_to_remove.append(ref)\n",
    "        else:\n",
    "            short_refs_to_keep.append(ref)\n",
    "\n",
    "    # COMPLETELY REMOVE\n",
    "    add_to_remove = ['produced by kanye west mike dean plain pat', 'Intro:', 'ENSEMBLE', 'JEFFERSON', 'Verse 2: Eminem', '[Chorus: KING GEORGE', '*Space Bar Tap*', 'BURR', 'LEE', '(Guitar Solo)', '(21st-Century schizoid man)']\n",
    "    # CHANGE/EDIT\n",
    "    edit_values = ['[HAMILTON]\\n No', '[HAMILTON]\\n Sir!', '[HAMILTON]\\n Ha', '[HAMILTON]\\n What?']\n",
    "    # OK\n",
    "    ok_keep = ['Mr. President', 'Mr. Vice President:', '“President John Adams”', 'Hamilton', 'Maty Noyes']\n",
    "\n",
    "    replace_dict = {'[HAMILTON]\\n No':'No', '[HAMILTON]\\n Sir!': 'Sir!', '[HAMILTON]\\n Ha': 'Ha', '[HAMILTON]\\n What?': 'What?'}\n",
    "\n",
    "    edit_idxs = []\n",
    "    for bad_ref in edit_values:\n",
    "        mask = df['ref_text'] == bad_ref\n",
    "        bad_idxs = list(df[mask].index)\n",
    "        for i in bad_idxs:\n",
    "            edit_idxs.append(i)\n",
    "\n",
    "    df['ref_text'].replace(replace_dict, inplace=True)\n",
    "\n",
    "    for i in add_to_remove:\n",
    "        tags_to_remove.append(i)\n",
    "        short_refs_to_keep.remove(i)\n",
    "\n",
    "    rt_idxs_to_drop = []\n",
    "    for bad_ref in tags_to_remove:\n",
    "        mask = df['ref_text'] == bad_ref\n",
    "        bad_idxs = list(df[mask].index)\n",
    "        for i in bad_idxs:\n",
    "            rt_idxs_to_drop.append(i)\n",
    "\n",
    "    df.drop(rt_idxs_to_drop, axis=0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def perform_ttsplit(df):\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "#     df_train.to_csv('../data/genius_data_train_319.csv')\n",
    "#     df_test.to_csv('../data/genius_data_test_319.csv')\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_tate_dfs(df_train, df_test):\n",
    "    ref_df_train = df_train[['ref_text', 'rt_id']]\n",
    "    tate_df_train = df_train[['tate_text', 'rt_id']]\n",
    "\n",
    "    ref_df_test = df_test[['ref_text', 'rt_id']]\n",
    "    tate_df_test = df_test[['tate_text', 'rt_id']]\n",
    "\n",
    "    ref_df_train.reset_index(drop=True, inplace=True)\n",
    "    tate_df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    ref_df_test.reset_index(drop=True, inplace=True)\n",
    "    tate_df_test.reset_index(drop=True, inplace=True)\n",
    "    # (tate_df_train['rt_id'] == ref_df_train['rt_id']).all()\n",
    "    # (tate_df_test['rt_id'] == ref_df_test['rt_id']).all()\n",
    "    return ref_df_train, tate_df_train, ref_df_test, tate_df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GONNA USE THIS TUTORIAL FOR REST OF ATTEMPT:\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb\n",
    "def isolate_corpuses(ref_df_train, tate_df_train, ref_df_test, tate_df_test):\n",
    "    refs_train = ref_df_train['ref_text']\n",
    "    tates_train = tate_df_train['tate_text']\n",
    "\n",
    "    refs_test = ref_df_test['ref_text']\n",
    "    tates_test = tate_df_test['tate_text']\n",
    "    return refs_train, refs_test, tates_train, tates_test\n",
    "\n",
    "def make_rt_doc_idx_dicts(ref_df_train, ref_df_test):\n",
    "    rt_to_doc_idx_train = ref_df_train['rt_id']\n",
    "    rt_to_doc_idx_test = ref_df_test['rt_id']\n",
    "\n",
    "    rt_doc_idx_train_dict = rt_to_doc_idx_train.to_dict()\n",
    "    rt_doc_idx_test_dict = rt_to_doc_idx_test.to_dict()\n",
    "    return rt_doc_idx_train_dict, rt_doc_idx_test_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(doc_series, tokens_only=False):\n",
    "    # with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "    for i, line in enumerate(doc_series):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "\n",
    "def get_rt_tt_corpuses(refs_train, refs_test, tates_train, tates_test):\n",
    "    train_tate_corpus = list(read_corpus(tates_train))\n",
    "    test_tate_corpus = list(read_corpus(tates_test, tokens_only=True))\n",
    "\n",
    "    train_refs_corpus = list(read_corpus(refs_train))\n",
    "    test_refs_corpus = list(read_corpus(refs_test, tokens_only=True))\n",
    "    return train_tate_corpus, test_tate_corpus, train_refs_corpus, test_refs_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(model, mod_corpus):\n",
    "    ranks = []\n",
    "    second_ranks = []\n",
    "    n_training_docs = len(mod_corpus)\n",
    "    for doc_id in range(n_training_docs):\n",
    "        inferred_vector = model.infer_vector(mod_corpus[doc_id].words)\n",
    "        sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "        rank = [docid for docid, sim in sims].index(doc_id)\n",
    "        ranks.append(rank)\n",
    "        second_ranks.append(sims[1])\n",
    "    # Let's count how each document ranks with respect to the training corpus\n",
    "    rank_counter = collections.Counter(ranks)  # Results vary between runs due to random seeding and very small corpus\n",
    "    cnt_correct_self_similarity_docs = rank_counter[0]\n",
    "    perc_correct_similarity = cnt_correct_self_similarity_docs / n_training_docs\n",
    "    greater_than_95 = perc_correct_similarity >= 0.95\n",
    "    return ranks, second_ranks, rank_counter, perc_correct_similarity, greater_than_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = get_and_clean_data()\n",
    "df_train, df_test = perform_ttsplit(cleaned_df)\n",
    "ref_df_train, tate_df_train, ref_df_test, tate_df_test = get_ref_tate_dfs(df_train, df_test)\n",
    "refs_train, refs_test, tates_train, tates_test = isolate_corpuses(ref_df_train, tate_df_train, ref_df_test, tate_df_test)\n",
    "rt_doc_idx_train_dict, rt_doc_idx_test_dict = make_rt_doc_idx_dicts(ref_df_train, ref_df_test)\n",
    "train_tate_corpus, test_tate_corpus, train_refs_corpus, test_refs_corpus = get_rt_tt_corpuses(refs_train, refs_test, tates_train, tates_test)\n",
    "\n",
    "ref_model1 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "ref_model2 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=80)\n",
    "ref_model3 = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=80)\n",
    "\n",
    "tate_model1 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "tate_model2 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=80)\n",
    "tate_model3 = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=80)\n",
    "\n",
    "rt_model1 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "rt_model2 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=80)\n",
    "rt_model3 = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=80)\n",
    "\n",
    "ref_models = [ref_model1, ref_model2, ref_model3]\n",
    "ref_mod_text = ['RM1(vs=50, mc=2, e=40)', 'RM2(vs=50, mc=2, e=80)', 'RM3(vs=100, mc=1, e=80)']\n",
    "tate_models = [tate_model1, tate_model2, tate_model3]\n",
    "tate_mod_text = ['TM1(vs=50, mc=2, e=40)', 'TM2(vs=50, mc=2, e=80)', 'TM3(vs=100, mc=1, e=80)']\n",
    "rt_models = [rt_model1, rt_model2, rt_model3]\n",
    "rt_mod_text = ['RTM1(vs=50, mc=2, e=40)', 'RTM2(vs=50, mc=2, e=80)', 'RTM3(vs=100, mc=1, e=80)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_examples(model, mod_corpus, doc_id):\n",
    "    inferred_vector = model.infer_vector(mod_corpus[doc_id])\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(mod_corpus[doc_id].words)))\n",
    "    print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "    for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "        print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(mod_corpus[sims[index][0]].words)))\n",
    "\n",
    "def compare_second_most_similar_doc_examples(mod_corpus, mod_second_ranks):\n",
    "    # We can run the next cell repeatedly to see a sampling other target-document comparisons.\n",
    "    # Pick a random document from the corpus and infer a vector from the model\n",
    "    doc_id = random.randint(0, len(mod_corpus) - 1)\n",
    "    # Compare and print the second-most-similar document\n",
    "    print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(mod_corpus[doc_id].words)))\n",
    "    sim_id = second_ranks[doc_id]\n",
    "    print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(modcorpus[sim_id[0]].words)))\n",
    "\n",
    "def random_model_tests(model, train_corpus, test_corpus):\n",
    "    # Testing the Model\n",
    "    # Using the same approach above, we'll infer the vector for a randomly chosen test document, and compare the document to our model by eye.\n",
    "    # Pick a random document from the test corpus and infer a vector from the model\n",
    "    doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "    inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    # Compare and print the most/median/least similar documents from the train corpus\n",
    "    print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "    print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "    for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "        print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_and_bottom_tate_docs(df, ref_df_train, ref_df_test):\n",
    "    # 3 rt_ids for annotations we know to be particularly \"bad\"\n",
    "    bottom_3_rtid = [rt_id for rt_id in df.nsmallest(3, 'votes_per_1000views')['rt_id']]\n",
    "    # 3 rt_ids for annotations we know to be \"good\", minus the one that's only annotating a '[VERSE]' tag\n",
    "    top_3_rtid = [rt_id for rt_id in df.nlargest(3, 'votes_per_1000views')['rt_id']]\n",
    "\n",
    "    b3_doc_id = []\n",
    "    b3_tot = []\n",
    "    for rt_id in bottom_3_rtid:\n",
    "        if rt_id in list(ref_df_train['rt_id']):    \n",
    "            mask = ref_df_train['rt_id'] == rt_id \n",
    "            b3_doc_id.append(ref_df_train[mask].index[0])\n",
    "            b3_tot.append('train')\n",
    "        else:\n",
    "            mask = ref_df_test['rt_id'] == rt_id\n",
    "            b3_doc_id.append(ref_df_test[mask].index[0])\n",
    "            b3_tot.append('test')\n",
    "\n",
    "    t3_doc_id = []\n",
    "    t3_tot = []\n",
    "    for rt_id in top_3_rtid:\n",
    "        if rt_id in list(ref_df_train['rt_id']):    \n",
    "            mask = ref_df_train['rt_id'] == rt_id \n",
    "            t3_doc_id.append(ref_df_train[mask].index[0])\n",
    "            t3_tot.append('train')\n",
    "        else:\n",
    "            mask = ref_df_test['rt_id'] == rt_id\n",
    "            t3_doc_id.append(ref_df_test[mask].index[0])\n",
    "            t3_tot.append('test')\n",
    "    return b3_doc_id, b3_tot, t3_doc_id, t3_tot\n",
    "\n",
    "def get_cos_sim_for_best_worst_rt_pairs(b3_doc_id, b3_tot, t3_doc_id, t3_tot, df, base_model, r_train_corpus, r_test_corpus, t_train_corpus, t_test_corpus):          \n",
    "    top = []\n",
    "    bottom = []\n",
    "    \n",
    "    for idx, b_doc_id in enumerate(b3_doc_id):\n",
    "        if b3_tot[idx] == 'train':\n",
    "            if b3_tot[idx] == 'train':\n",
    "                r_inf_vec = base_model.infer_vector(r_train_corpus[b_doc_id].words, epochs=base_model.epochs).reshape(-1, 1)\n",
    "                t_inf_vec = base_model.infer_vector(t_train_corpus[b_doc_id].words, epochs=base_model.epochs).reshape(-1, 1)\n",
    "            else:\n",
    "                r_inf_vec = base_model.infer_vector(r_test_corpus[b_doc_id], epochs=base_model.epochs).reshape(-1, 1)\n",
    "                t_inf_vec = base_model.infer_vector(t_test_corpus[b_doc_id], epochs=base_model.epochs).reshape(-1, 1)\n",
    "            # might need to just do straight up np.cosine similarity calc between vecs\n",
    "            rt_iv_cs = 1 - cosine(r_inf_vec, t_inf_vec)\n",
    "            bottom.append(rt_iv_cs)\n",
    "#         bottom.append([mean_cs, bad_cs])\n",
    "\n",
    "    for idx, t_doc_id in enumerate(t3_doc_id):\n",
    "        if t3_tot[idx] == 'train':\n",
    "            if t3_tot[idx] == 'train':\n",
    "                r_inf_vec = base_model.infer_vector(r_train_corpus[t_doc_id].words, epochs=base_model.epochs).reshape(-1, 1)\n",
    "                t_inf_vec = base_model.infer_vector(t_train_corpus[t_doc_id].words, epochs=base_model.epochs).reshape(-1, 1)\n",
    "            else:\n",
    "                r_inf_vec = base_model.infer_vector(r_test_corpus[t_doc_id], epochs=base_model.epochs).reshape(-1, 1)\n",
    "                t_inf_vec = base_model.infer_vector(t_test_corpus[t_doc_id], epochs=base_model.epochs).reshape(-1, 1)\n",
    "            # might need to just do straight up np.cosine similarity calc between vecs\n",
    "            rt_iv_cs = 1 - cosine(r_inf_vec, t_inf_vec)\n",
    "            top.append(rt_iv_cs)\n",
    "#         top.append([mean_cs, good_cs])\n",
    "    \n",
    "    return top, bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2674, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(669, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_rm1 = get_tmpfile(\"doc2vec_rm1\")\n",
    "fname_rm2 = get_tmpfile(\"doc2vec_rm2\")\n",
    "fname_rm3 = get_tmpfile(\"doc2vec_rm3\")\n",
    "\n",
    "fname_tm1 = get_tmpfile(\"doc2vec_tm1\")\n",
    "fname_tm2 = get_tmpfile(\"doc2vec_tm2\")\n",
    "fname_tm3 = get_tmpfile(\"doc2vec_tm3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3_doc_id, b3_tot, t3_doc_id, t3_tot = get_top_and_bottom_tate_docs(cleaned_df, ref_df_train, ref_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING: RM1(vs=50, mc=2, e=40)\n",
      "CPU times: user 3.42 s, sys: 845 ms, total: 4.26 s\n",
      "Wall time: 2.88 s\n",
      "Model Self-Similarity Test Passed?: False\n",
      "Model % Self-Similar: 0.8799551234106208\n",
      "Test Document (517): «know»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (2491, 0.9033986926078796): «but»\n",
      "\n",
      "MEDIAN (2446, 0.5495301485061646): «produced by young chop co production by kanye west noah goldstein and the twilite tone»\n",
      "\n",
      "LEAST (2643, -0.49779078364372253): «ayy got somethin hol up we gon function no assumptions»\n",
      "\n",
      "Mean of CS between BEST 3 Annotations: 0.44861310720443726\n",
      "[0.44861310720443726]\n",
      "Mean of CS between WORST 3 Annotations: 0.1788015173127254\n",
      "[0.48971104621887207, -0.015330487862229347, 0.06202399358153343]\n",
      "NOW, ONTO THE NEXT MODEL!\n",
      "EVALUATING: RM2(vs=50, mc=2, e=80)\n",
      "CPU times: user 7.64 s, sys: 1.77 s, total: 9.41 s\n",
      "Wall time: 6.06 s\n",
      "Model Self-Similarity Test Passed?: False\n",
      "Model % Self-Similar: 0.9323111443530292\n",
      "Test Document (68): «produced by kanye west mike dean plain pat»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (56, 0.9533003568649292): «produced by kanye west rza»\n",
      "\n",
      "MEDIAN (1122, 0.3965378999710083): «shawty look good in the moonlight all these pussy niggas so bad mind»\n",
      "\n",
      "LEAST (2643, -0.3736002445220947): «ayy got somethin hol up we gon function no assumptions»\n",
      "\n",
      "Mean of CS between BEST 3 Annotations: 0.24981649219989777\n",
      "[0.24981649219989777]\n",
      "Mean of CS between WORST 3 Annotations: 0.17323681463797888\n",
      "[0.3280223309993744, 0.07068956643342972, 0.12099854648113251]\n",
      "NOW, ONTO THE NEXT MODEL!\n",
      "EVALUATING: RM3(vs=100, mc=1, e=80)\n",
      "CPU times: user 8.13 s, sys: 1.85 s, total: 9.98 s\n",
      "Wall time: 6.49 s\n",
      "Model Self-Similarity Test Passed?: False\n",
      "Model % Self-Similar: 0.9330590875093493\n",
      "Test Document (97): «all these thots on christian mingle almost what got tristan single if you don ball like him or kobe guarantee that bitch gonna leave you»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d100,n5,w5,s0.001,t3):\n",
      "\n",
      "MOST (2109, 0.38400930166244507): «that poor man they re gonna eat him alive»\n",
      "\n",
      "MEDIAN (1850, -0.11348816007375717): «already know that life is deep but still dig her»\n",
      "\n",
      "LEAST (1916, -0.43850481510162354): «paint picture of my pain for the world to see»\n",
      "\n",
      "Mean of CS between BEST 3 Annotations: 0.22279098629951477\n",
      "[0.22279098629951477]\n",
      "Mean of CS between WORST 3 Annotations: 0.11394616961479187\n",
      "[0.329649955034256, 0.08271713554859161, -0.07052858173847198]\n",
      "NOW, ONTO THE NEXT MODEL!\n"
     ]
    }
   ],
   "source": [
    "for idx, ref_mod in enumerate(ref_models):\n",
    "    model_header = ref_mod_text[idx]\n",
    "    print(\"EVALUATING:\", model_header)\n",
    "    ref_mod.build_vocab(train_refs_corpus)\n",
    "    %time ref_mod.train(train_refs_corpus, total_examples=ref_mod.corpus_count, epochs=ref_mod.epochs)\n",
    "\n",
    "    mod_ranks, mod_second_ranks, mod_rank_counter, perc_correct_similarity, greater_than_95 = assess_model(ref_mod, train_refs_corpus)\n",
    "    print(\"Model Self-Similarity Test Passed?:\", greater_than_95)\n",
    "    print(\"Model % Self-Similar:\", perc_correct_similarity)\n",
    "\n",
    "    doc_id = 4\n",
    "#     print_most_similar_examples(ref_mod, train_refs_corpus, doc_id)\n",
    "#     compare_second_most_similar_doc_examples(train_refs_corpus, mod_second_ranks)\n",
    "\n",
    "    random_model_tests(ref_mod, train_refs_corpus, test_refs_corpus)\n",
    "    \n",
    "    top, bottom = get_cos_sim_for_best_worst_rt_pairs(b3_doc_id, b3_tot, t3_doc_id, t3_tot, cleaned_df, ref_mod, train_refs_corpus, test_refs_corpus, train_tate_corpus, test_tate_corpus)\n",
    "    print(\"Mean of CS between BEST 3 Annotations:\", np.array(top).mean())\n",
    "    print(top)\n",
    "    print(\"Mean of CS between WORST 3 Annotations:\", np.array(bottom).mean())\n",
    "    print(bottom)\n",
    "    print('NOW, ONTO THE NEXT MODEL!')\n",
    "\n",
    "# ref_model1.save(fname_rm1)\n",
    "# ref_model2.save(fname_rm2)\n",
    "# ref_model3.save(fname_rm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING: TM1(vs=50, mc=2, e=40)\n",
      "CPU times: user 12.9 s, sys: 1.21 s, total: 14.1 s\n",
      "Wall time: 6.45 s\n",
      "Model Self-Similarity Test Passed?: True\n",
      "Model % Self-Similar: 0.9973821989528796\n",
      "Test Document (308): «in october women accused american film producer harvey weinstein of rape sexual assault and sexual abuse over period of at least years over eighty women have since come forward and accused weinstein he was arrested in may and released on bail according to reports weinstein invited young actresses or models into hotel room or office on the pretext of discussing their career and then demanded massages or sex the accusations resulted in other women coming forward with their own experiences concerning sexual harassment and rape on social media under the hashtag metoo»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (1109, 0.5570858716964722): «you may not be able to turn bad girl good but jay career is testament to the male equivalent being possible most criticism surrounding their relationship stemmed from jay nefarious past and beyoncé clean cut image gossip sites were awash with stories of jay alleged infidelity and of rift with her father claiming beyoncé immaturity led her to court the affections of dangerous men this is defiant statement my own woman and ve chosen the right person time is the great purifier the black hour glass signifies the passing of dirty time moving towards pure future clichés surrounding thug loving are made to be broken»\n",
      "\n",
      "MEDIAN (1384, 0.17056897282600403): «smoke means to beef someone wayne boasts lot about always being ready for fight and not being easily defeated so this means you should never try to smoke wayne unless you re literally lighting cigarette»\n",
      "\n",
      "LEAST (1329, -0.3378186821937561): «practice makes perfect is popular idiom that expresses the idea that proficiency in certain skill can only be achieved if one exercises it frequently cole had previously touched on the idea of perfection in his song with missy elliott nobody perfect which interestingly references another expression nobody perfect uh nobody perfect ay hey but you re perfect for me cole makes cheeky assertion that he is perfect he creates play on words with the idea that if perfection was the offspring of practice if practice made perfect then cole would be perfection»\n",
      "\n",
      "Mean of CS between BEST 3 Annotations: 0.13199958205223083\n",
      "[0.13199958205223083]\n",
      "Mean of CS between WORST 3 Annotations: 0.4535595675309499\n",
      "[0.510839581489563, 0.24215289950370789, 0.6076862215995789]\n",
      "NOW, ONTO THE NEXT MODEL!\n",
      "EVALUATING: TM2(vs=50, mc=2, e=80)\n",
      "CPU times: user 24.4 s, sys: 2.28 s, total: 26.7 s\n",
      "Wall time: 12.1 s\n",
      "Model Self-Similarity Test Passed?: True\n",
      "Model % Self-Similar: 0.9970082273747195\n",
      "Test Document (122): «pursuit of your goals can make you feel tired and lead to you considering options like quitting until you hear this song and realize that you have to keep going till you collapse»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (899, 0.5440811514854431): «hamilton alludes to time indirectly occasionally in the show certainly he wishes he had more of it but he surprised he got this much see never thought get past twenty where come from some get half as many ask anybody why we livin fast and we laugh reach for flask we have to make this moment last that plenty»\n",
      "\n",
      "MEDIAN (573, 0.11998432874679565): «the way says this is similar in style to the ad lib often used by ki mask the slump god and is most likely used in this freestyle to reference to him»\n",
      "\n",
      "LEAST (2396, -0.32917487621307373): «beyoncé tells the cheating man to suck on my balls she showing him the same disrespect he shown her of course as woman telling man to suck my balls is also statement of strength and masculinity whatever power this man once held beyoncé now taken for herself she pays him no mind these lyrics appear to mock part of jay verse on kanye west so appalled fresher than you all so don have to pause all of all can suck my balls through my drawers saying pause after saying something sexually suggestive is little meme in its own right the use of the term originated in harlem as way to dissociate oneself from something that could inadvertently sound like piece of homosexual bawdy it was popularized along with the more blatantly offensive no homo by harlem rappers dipset the boondocks animated series also used pause as recurring gag while the pause no homo game has been widely condemned as homophobic bey is showing off winking irony by using the term here sorry not sorry»\n",
      "\n",
      "Mean of CS between BEST 3 Annotations: 0.10255170613527298\n",
      "[0.10255170613527298]\n",
      "Mean of CS between WORST 3 Annotations: 0.38649188975493115\n",
      "[0.3079913854598999, 0.24198035895824432, 0.6095039248466492]\n",
      "NOW, ONTO THE NEXT MODEL!\n",
      "EVALUATING: TM3(vs=100, mc=1, e=80)\n",
      "CPU times: user 25.2 s, sys: 2.13 s, total: 27.4 s\n",
      "Wall time: 12 s\n",
      "Model Self-Similarity Test Passed?: True\n",
      "Model % Self-Similar: 0.9977561705310396\n",
      "Test Document (20): «bring to task is an idiom meaning to punish for wrongdoing here jefferson is calling for someone to make hamilton pay for his actions within washington cabinet as the treasury secretary while hamilton continues to badger congress to further his debt plan members of the opposing party the democratic republicans are becoming fed up with him madison burr jefferson finally take it upon themselves to see hamilton punished»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d100,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (500, 0.4757940173149109): «while it makes sense that he be less than happy with burr hamilton incredulity here is also ironic throughout the story hamilton has accused burr of standing for nothing in the room where it happens he tells burr you get nothing if you wait for it but when burr finally stops waiting for it hamilton not happy about it hamilton also being hypocrite in the room where it happens we see hamilton engaging in some dubious politics and making compromises in order to get what he wants he seizes the opportunity to get his debt plan approved by trading away the location of the capital that said hamilton wasn really compromising his underlying principles by making this deal while burr casual switching of parties makes him seem like he has no principles»\n",
      "\n",
      "MEDIAN (1772, 0.10796795040369034): «while kanye success is nothing new he still has to deal with people being friends with him for his wealth whether it gold digger or just person wanting something kanye unsure who his real friends are eminem has explored similar uncertainty to who his friends are on his song if had what are friends friends are people that you think are your friends but they re really your enemies with secret identities and disguises to hide their true colors so just when you think you re close enough to be brothers they wanna come back and cut your throat when you ain looking»\n",
      "\n",
      "LEAST (2022, -0.23176920413970947): «ariana achieved breakthrough in her career in when she was only going from relatively normal life straight into stardom as singer and actress can be overwhelming the terrorist attack in may after one of her concerts in manchester united kingdom further complicated things it caused her to have anxiety attacks and experience disorder that makes things seem unreal the person who took her mind off of all of this could have been her boyfriend at the time mac miller in the first pictures of ariana after the news of the bombing she can be seen getting off the plane and emotionally embracing miller mac seemed to have been calming force for ariana anxiety in his track dunno speculated to be penned about ariana he states and you you don gotta work harder can calm you down»\n",
      "\n",
      "Mean of CS between BEST 3 Annotations: 0.105513796210289\n",
      "[0.105513796210289]\n",
      "Mean of CS between WORST 3 Annotations: 0.32702865203221637\n",
      "[0.4066063463687897, 0.058017879724502563, 0.5164617300033569]\n",
      "NOW, ONTO THE NEXT MODEL!\n"
     ]
    }
   ],
   "source": [
    "for idx, tate_mod in enumerate(tate_models):\n",
    "    model_header = tate_mod_text[idx]\n",
    "    print(\"EVALUATING:\", model_header)\n",
    "    tate_mod.build_vocab(train_tate_corpus)\n",
    "    %time tate_mod.train(train_tate_corpus, total_examples=tate_mod.corpus_count, epochs=tate_mod.epochs)\n",
    "\n",
    "    mod_ranks, mod_second_ranks, mod_rank_counter, perc_correct_similarity, greater_than_95 = assess_model(tate_mod, train_tate_corpus)\n",
    "    print(\"Model Self-Similarity Test Passed?:\", greater_than_95)\n",
    "    print(\"Model % Self-Similar:\", perc_correct_similarity)\n",
    "\n",
    "    doc_id = 4\n",
    "#     print_most_similar_examples(ref_mod, train_refs_corpus, doc_id)\n",
    "#     compare_second_most_similar_doc_examples(train_refs_corpus, mod_second_ranks)\n",
    "\n",
    "    random_model_tests(tate_mod, train_tate_corpus, test_tate_corpus)\n",
    "\n",
    "    \n",
    "    top, bottom = get_cos_sim_for_best_worst_rt_pairs(b3_doc_id, b3_tot, t3_doc_id, t3_tot, cleaned_df, tate_mod, train_refs_corpus, test_refs_corpus, train_tate_corpus, test_tate_corpus)\n",
    "    print(\"Mean of CS between BEST 3 Annotations:\", np.array(top).mean())\n",
    "    print(top)\n",
    "    print(\"Mean of CS between WORST 3 Annotations:\", np.array(bottom).mean())\n",
    "    print(bottom)\n",
    "    print('NOW, ONTO THE NEXT MODEL!')\n",
    "    \n",
    "    \n",
    "# tate_model1.save(fname_tm1)\n",
    "# tate_model2.save(fname_tm2)\n",
    "# tate_model3.save(fname_tm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Persist a model to disk\n",
    "# fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "# model.save(fname)\n",
    "# model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
